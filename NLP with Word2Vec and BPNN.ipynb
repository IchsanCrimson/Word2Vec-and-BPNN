{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2301888682 - ICHSAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel like I am drowning. #depression #anxiet...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#panic Panic attack from fear of starting new ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My bus was in a car crash... I'm still shaking...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's the #FirstDayofFall and I'm so happy. Sip...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Morning all! Of course it is sunny on this Mon...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data Label\n",
       "0  I feel like I am drowning. #depression #anxiet...  fear\n",
       "1  #panic Panic attack from fear of starting new ...  fear\n",
       "2  My bus was in a car crash... I'm still shaking...  fear\n",
       "3  Just got back from seeing @GaryDelaney in Burs...   joy\n",
       "4  It's the #FirstDayofFall and I'm so happy. Sip...   joy\n",
       "5  Morning all! Of course it is sunny on this Mon...   joy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = [\"Data\", \"Label\"],\n",
    "                 data = [[\"I feel like I am drowning. #depression #anxiety #failure #worthless\", \"fear\"],\n",
    "                        [\"#panic Panic attack from fear of starting new medication\", \"fear\"],\n",
    "                        [\"My bus was in a car crash... I'm still shaking a bit... This week was an absolute horror and this was the icing on the cake... #terrible\", \"fear\"],\n",
    "                        [\"Just got back from seeing @GaryDelaney in Burslem. AMAZING!! Face still hurts from laughing so much #hilarious\", \"joy\"],\n",
    "                        [\"It's the #FirstDayofFall and I'm so happy. Sipping my #PumpkinSpice flavoured coffee and #smiling! Happy Fall everyone! #amwriting\", \"joy\"],\n",
    "                        [\"Morning all! Of course it is sunny on this Monday morning to cheerfully welcome us back to work.:)\", \"joy\"]])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Data Preprocessing\n",
    "# a) Preprocess Sentence (X)\n",
    "## 1. Tokenize menggunakan .split() \n",
    "### - Tujuan: agar dapat membuat sentence menjadi per word token.\n",
    "## 2. Lowercase menggunakan .lower() \n",
    "### - Dalam sentiment analysis, melihat kata yang digunakan saja sudah cukup. \n",
    "### - Contoh: \"AMAZING!\" dan \"amazing!\", tidak peduli huruf besar atau kecil, yang penting keyword \"amazing\" nya lah yang membuatnya tergolong \"joy\".\n",
    "## 3. Remove Stop Words \n",
    "### - Yang memiliki arti sentiment hanya keyword yang bukan stop words. \n",
    "### - Contoh: \"am\", \"was\" kita tidak bisa tahu mana \"joy\" atau \"fear\", kita hanya butuh keyword yang bukan stopword seperti \"amazing\", \"horror\".\n",
    "## 4. Padding Cutting \n",
    "### - Setiap sentence memiliki panjang yang berbeda, maka perlu disamakan semuanya. \n",
    "### - Semua sentence akan dibuat sama panjang yaitu average dari panjang semua kalimat atau 10. \n",
    "### - Alasan mengambil panjang average: kalau mengambil panjang max, data traning sangat sedikit dan kebanyakan sentence label \"fear\" yang pendek akan mendapat banyak padding. Hal ini akan merusak model karena model akan menganggap sentence  yang banyak padding cenderung kategori \"fear\".\n",
    "### - Padding dengan menambahkan (\"\").\n",
    "## Alasan saya tidak menggunakan stemming adalah ketika saya melakukan stemming dengan bantuan nltk.stem PorterStemmer, hasilnya kurang baik seperti \"depression\" menjadi \"depressi\".\n",
    "# b) Preprocess Label (Y)\n",
    "## 1. Label hanya berupa binary classification (\"fear\", \"joy\"), jadi OrdinalEncoder sudah cukup karena bisa direpresentasikan oleh 0 dan 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['feel',\n",
       "   'like',\n",
       "   'drowning.',\n",
       "   '#depression',\n",
       "   '#anxiety',\n",
       "   '#failure',\n",
       "   '#worthless',\n",
       "   '',\n",
       "   '',\n",
       "   ''],\n",
       "  ['#panic',\n",
       "   'panic',\n",
       "   'attack',\n",
       "   'fear',\n",
       "   'starting',\n",
       "   'new',\n",
       "   'medication',\n",
       "   '',\n",
       "   '',\n",
       "   ''],\n",
       "  ['bus',\n",
       "   'car',\n",
       "   'crash...',\n",
       "   \"i'm\",\n",
       "   'shaking',\n",
       "   'bit...',\n",
       "   'week',\n",
       "   'absolute',\n",
       "   'horror',\n",
       "   'icing'],\n",
       "  ['got',\n",
       "   'seeing',\n",
       "   '@garydelaney',\n",
       "   'burslem.',\n",
       "   'amazing!!',\n",
       "   'face',\n",
       "   'hurts',\n",
       "   'laughing',\n",
       "   '#hilarious',\n",
       "   ''],\n",
       "  [\"it's\",\n",
       "   '#firstdayoffall',\n",
       "   \"i'm\",\n",
       "   'happy.',\n",
       "   'sipping',\n",
       "   '#pumpkinspice',\n",
       "   'flavoured',\n",
       "   'coffee',\n",
       "   '#smiling!',\n",
       "   'happy'],\n",
       "  ['morning',\n",
       "   'all!',\n",
       "   'course',\n",
       "   'sunny',\n",
       "   'monday',\n",
       "   'morning',\n",
       "   'cheerfully',\n",
       "   'welcome',\n",
       "   'work.:)',\n",
       "   '']],\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_cut(data, max_length):\n",
    "    for i, sentence in enumerate(data):\n",
    "        length = len(sentence)\n",
    "        # padding\n",
    "        if length < max_length:\n",
    "            for j in range(length, max_length): data[i].append(\"\")\n",
    "        # cutting\n",
    "        else:\n",
    "            data[i] = data[i][:max_length]\n",
    "    return data\n",
    "\n",
    "def preprocess(data):\n",
    "    MAX_LENGTH = 10\n",
    "    # Remove Stop Words, Lowercase, Tokenize\n",
    "    data = [remove_stopwords(sentence.lower()).split() for sentence in data]\n",
    "    # Padding or Cutting into MAX_LENGTH\n",
    "    data = pad_cut(data, MAX_LENGTH)\n",
    "    return data\n",
    "\n",
    "x = preprocess(df[\"Data\"])\n",
    "\n",
    "y = df[[\"Label\"]]\n",
    "scaler_y = OrdinalEncoder().fit(y)\n",
    "y = scaler_y.transform(y)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Word Embedding as Feature, number of hashtag as Feature\n",
    "# a) Word Embedding as Feature\n",
    "## 1. Cara kerja Model Word2Vec Gensim\n",
    "### - Cara kerja word2vec adalah dengan membangun neural network (Input - Hidden - Output).\n",
    "### - By default, word2vec dari gensim adalah CBOW. (Referensi: https://groups.google.com/g/gensim/c/neDSgepSpCk?pli=1)\n",
    "### - Konsep CBOW adalah melatih neural network dengan: Input (pair word bersebelahan dengan target word sejauh window_size) dan Output (target word).\n",
    "### - Contoh:\n",
    "#### window_size = 1, sentence = \"I like you bro\"\n",
    "#### Maka Training berupa:\n",
    "#### \"I like\", Target Word = \"I\", sisanya pair word input (bersebelahan sejauh windowsize).\n",
    "#### Input: (I, like), Output = (I)\n",
    "#### \"I like you\", Target Word = \"like\", sisanya pair wod input (bersebelahan sejauh windowsize).\n",
    "#### Input: (like, I), Output = (like)\n",
    "#### Input: (like, you), Output = (like)\n",
    "#### \"like you bro\", Target Word = \"you\", sisanya pair wod input (bersebelahan sejauh windowsize).\n",
    "#### Input: (you, like), Output = (you)\n",
    "#### Input: (you, bro), Output = (you)\n",
    "#### \"you bro\", Target Word = \"bro\", sisanya pair wod input (bersebelahan sejauh windowsize).\n",
    "#### Input: (bro, you), Output = (bro)\n",
    "### - Setelah melakukan training denga input, output diatas yang merupakan konsep CBOW, maka weight antara Input Layer menuju Hidden Layer akan dijadikan sebagai representasi vector untuk word tersebut. (Referensei: https://israelg99.github.io/2017-03-23-Word2Vec-Explained/)\n",
    "### - Tujuan digunakan weight Input Layer menuju Hidden Layer adalah karena ketika proses training, nilai weight sudah beradaptasi untuk merepresentasi data corpus trainingnya.\n",
    "### - Untuk cara kerja ANN hanya berupa update weight sesuai error antara prediksi model dan real value.\n",
    "## 2. Mengapa menggunakan word2vec pada masalah ini?\n",
    "### - Neural Network tidak mengenal huruf dan hanya mengenal angka sehingga Word2Vec dapat digunakan untuk representasi word dalam bentuk vector angka.\n",
    "### - Dengan representasi vector angka word2vec dapat merepresentasikan word similarity.\n",
    "### - Feature word similarity untuk model akan sangat bermanfaat seperti jika dalam satu sentence banyak similarity word yang dekat maka akan lebih jelas hasilnya mengarah ke label mana.\n",
    "# b) Number of Hashtags as Feature\n",
    "## 1. Mengapa menggunakan number of hashtags sebagai feature? \n",
    "### - Permintaan dari soal, jadi saya ikuti. \n",
    "### - Menurut saya, feature ini tidak akan membawa peningkatan performa karena tidak ada hubungan antara jumlah Hashtags dan \"Fear\" \"Joy\" (saya mencobanya dan benar feature ini tidak membawa peningkatan performa). \n",
    "### - Contoh: \"#Happy\" dan \"#Sad\", keduanya memiliki jumlah hashtags = 1, akan tetapi kita tidak dapat menilai mana joy dan fear hanya dari jumlah hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of hashtags on each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 0, 1, 3, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tag = []\n",
    "\n",
    "for sentence in x:\n",
    "    tag = 0\n",
    "    for word in sentence:\n",
    "        if \"#\" in word: tag += 1\n",
    "    num_tag.append(tag)\n",
    "    \n",
    "num_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer \n",
    "### - Convert Word menjadi vector representation menggunakan word2vec\n",
    "### - Reshape menjadi shape yang diterima BPNN dari shape (6 sentences, 10 words, 100 vector representation) menjadi shape (6, 10 word * 100 vector) = (6, 1000)\n",
    "### - Append Number of hashtags sebagai feature (6, 1000 + 1 Number of hashtags) = (6, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation:\n",
      " [[ 0.00242733 -0.00378142  0.00402846 ... -0.00027745  0.00397951\n",
      "   0.00075789]\n",
      " [ 0.00122478 -0.00354308 -0.00489287 ... -0.00027745  0.00397951\n",
      "   0.00075789]\n",
      " [-0.00408636  0.00057664 -0.00444802 ... -0.00345904  0.00466163\n",
      "  -0.00051724]\n",
      " [-0.00335128  0.00338272  0.00468503 ... -0.00027745  0.00397951\n",
      "   0.00075789]\n",
      " [ 0.00228935 -0.00158379 -0.00145577 ...  0.00390947 -0.00162096\n",
      "  -0.0020406 ]\n",
      " [ 0.00331802 -0.00270061  0.00328912 ... -0.00027745  0.00397951\n",
      "   0.00075789]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 1001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(data):\n",
    "    # membangun model word2vec untuk setiap kata yang muncul minimal 1 kali\n",
    "    # by default, vector_size = 100 dan window_size = 5\n",
    "    # vector_size = 100 berarti representasi suatu word akan sebanyak 100 vector angka\n",
    "    word2vec = Word2Vec(data, min_count=1)\n",
    "    \n",
    "    # mengubah setiap word menjadi representasi vectornya\n",
    "    data = [[word2vec.wv[word] for word in sentence] for sentence in data]\n",
    "    # convert ke numpy array untuk akses shape\n",
    "    data = np.array(data)\n",
    "    # reshape ke bentuk flatten untuk input ke BPNN\n",
    "    data = data.reshape(-1, data.shape[1]*data.shape[2])\n",
    "    return data\n",
    "\n",
    "# Input representasi word vector\n",
    "x_vec = vectorize(x)\n",
    "print(\"Vector representation:\\n\", x_vec)\n",
    "\n",
    "# Menambah Number of Hashtags sebagai feature\n",
    "x_vec = [np.append(vec, num_tag[i]) for i, vec in enumerate(x_vec)]\n",
    "# convert ke numpy array untuk akses shape\n",
    "x_vec = np.array(x_vec)\n",
    "x_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Train 4 data (2 Fear, 2 Joy) dan Test 2 data (1 Fear, 1 Joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 1001), (4, 1), (2, 1001), (2, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [1, 2, 4, 5]\n",
    "test = [0, 3]\n",
    "\n",
    "x_train = np.array(x_vec[train])\n",
    "y_train = np.array(y[train])\n",
    "x_test = np.array(x_vec[test])\n",
    "y_test = np.array(y[test])\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Build Model BPNN\n",
    "## a) Model BPNN\n",
    "### 1. Architecture\n",
    "#### - Input Layer (1001 neuron) \n",
    "#### - Hidden Layer 1 (500 neuron) \n",
    "#### - Hidden Layer 2 (100 neuron) \n",
    "#### - Hidden Layer 3 (25 neuron) \n",
    "#### - Output Layer (1 neuron)\n",
    "### 2. Input dan Output\n",
    "#### - Input menggunakan data word vector hasil word2vec yang sudah di flatten + number of hashtags dalam sentence\n",
    "#### - Output menggunakan 0 dan 1 untuk representasi \"fear\" dan \"joy\"\n",
    "### 3. Training Process\n",
    "#### - Hidden Layer 1-3 tidak menggunakan activation function, agar hasil training tidak di compress dan nilai word similarity akan tetap terjaga dengan baik.\n",
    "#### - Output Layer menggunakan Sigmoid agar hasil berada dalam range 0 sampai 1 sesuai Output yang diinginkan.\n",
    "#### - Sigmoid juga dinyatakan bagus dalam binary classification.\n",
    "#### - Selebihnya architecture merupakan hasil eksperimen / coba-coba untuk memperoleh model yang baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 780ms/step - loss: 0.6904 - accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6138 - accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5817 - accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5580 - accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5264 - accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4870 - accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4440 - accuracy: 0.7500\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4014 - accuracy: 0.7500\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3613 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3233 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2844 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2423 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1981 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1551 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1170 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0617 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0439 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0216 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x269359dd880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(500))\n",
    "    model.add(layers.Dense(100))\n",
    "    model.add(layers.Dense(25))\n",
    "    model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", \"accuracy\")\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Performance of BPNN: accuracy, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(y_pred, y):\n",
    "    # convert < 0.5 jadi 0\n",
    "    # convert >= 0.5 jadi 1\n",
    "    y_pred = y_pred.round()\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print(f\"Accuracy: {accuracy_score(y, y_pred)*100}%\")\n",
    "    print(f\"Precision: {precision_score(y, y_pred)*100}%\")\n",
    "    print(f\"Recall: {recall_score(y, y_pred)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Training Data\n",
    "### Seperti biasa, tentu saja training data akan mendapat nilai 100%. Hal ini terjadi karena data corpus terlalu sedikit sehingga model menjadi overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy: 100.0%\n",
      "Precision: 100.0%\n",
      "Recall: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data\")\n",
    "y_pred = model.predict(x_train)\n",
    "performance(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Testing Data\n",
    "### Dapat dilihat data testing tidak diprediksi dengan sempurna, hal ini menunjukkan overfitting kemungkinan besar terjadi.\n",
    "### Selain itu, tidak semua corpus pada data test ada dalam corpus data train, sehingga juga mempengaruhi performa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data\n",
      "Confusion Matrix:\n",
      "[[0 1]\n",
      " [0 1]]\n",
      "Accuracy: 50.0%\n",
      "Precision: 50.0%\n",
      "Recall: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Data\")\n",
    "y_pred = model.predict(x_test)\n",
    "performance(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Propose application\n",
    "## Saya dapat mengumpulkan berita berhubungan dengan saham atau crypto dengan keyword tertentu dan melakukan analisa sentimentnya untuk mengetahui gambaran sentiment pasar saat ini.\n",
    "## Dapat juga digunakan untuk mendapatkan gambaran review dari keseluruhan penilaian customer dari perusahaan sehingga tidak perlu cek satu per satu untuk mengetahui apakah pelayanan perusahaan tersebut sudah maksimal atau tidak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
